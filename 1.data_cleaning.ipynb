{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take raw i3 inputs and generate clean and formatted data\n",
    "Kathleen Kennedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import all data\n",
    "# i3 raw data\n",
    "ven_full_df = pd.read_csv('inputs/companies-export-18919.csv') # 2022 data\n",
    "ipos_full_df = pd.read_csv('inputs/ipos-export-18925.csv') # 2022 data\n",
    "ma_full_df = pd.read_csv('inputs/ma-export-18923.csv') # 2022 data\n",
    "rel_full_df = pd.read_csv('inputs/relationships-export-18924.csv') # 2022 data\n",
    "\n",
    "# data updates from Susanne Kurowski and Claudia Doblinger\n",
    "ven_updates = pd.read_csv('inputs/updated_venture_data.csv')  # Binary failure and MA updates \n",
    "sus_df = pd.read_csv('inputs/foundingyr2.csv') # Year founded updates \n",
    "\n",
    "# data updates from author team\n",
    "ven_founding_updates = pd.read_csv('inputs/companies-missing-year.csv') # Year founded updates \n",
    "ven_founding_updates = ven_founding_updates[['company','year.founded','year.founded.update']]\n",
    "ven_fail_updates = pd.read_csv('inputs/venture_failure_updates.csv') # Year of failure updates \n",
    "ven_fail_updates = ven_fail_updates[['Company','Status_update','Failure Date_update',\n",
    "                                     'Failure Date Source']].drop_duplicates()\n",
    "inactive_updates = pd.read_csv('inputs/inactive_2-5yrs.csv') # Status updates\n",
    "inactive_updates = inactive_updates[['Company','Status_update','Failure_date','IPO_date',\n",
    "                                     'MA_date','Acquirer_update']]\n",
    "\n",
    "ven_location_updates = pd.read_csv('inputs/author_location_updates.csv') # Location updates \n",
    "rel_updates = pd.read_csv('inputs/rel_date_update.csv') # Customer relationship updates\n",
    "rel_updates = rel_updates[['Company','Relationship Partner','Relationship Date Update']]\n",
    "rel_updates = rel_updates.dropna(subset=['Relationship Date Update'])\n",
    "\n",
    "# companies to exclude based on relavancy filters in 0.preprocessing.R\n",
    "excluded_df = pd.read_csv('inputs/companies-exclude-20220805.csv') # 2022 data\n",
    "excluded = excluded_df['company'].to_list()\n",
    "\n",
    "# cleaned investors data from 1.dataprocessing.R\n",
    "cvc_df = pd.read_csv('inputs/investors_20220805.csv').drop_duplicates()\n",
    "cvc_df = cvc_df.drop(['Unnamed: 0','i3.url','year.founded', 'industry.group', 'sector', 'tags',\n",
    "                      'short.description', 'state', 'country', 'region',],axis=1)\n",
    "cvc_df = cvc_df.fillna(value={'investment.year':-1})\n",
    "cvc_df = cvc_df[cvc_df['investment.year'] < 2022]\n",
    "\n",
    "# patent data\n",
    "pat_df = pd.read_csv('inputs/patent_counts_2.csv') # output from matching_patents.R?\n",
    "\n",
    "# ARPA-E data - prime awards and subawards from usaspending.gov, manually matched by authors\n",
    "arpae_new = pd.read_csv('inputs/arpae_new.csv') \n",
    "arpae_upd = pd.read_csv('inputs/arpae_updates.csv')\n",
    "\n",
    "# SBIR data - awards from SBIR API, output from sbir_award_search.R and sbir_i3_match.R\n",
    "sbir_df = pd.read_csv('outputs/sbir_matches.csv')\n",
    "\n",
    "#total_df = pd.read_csv('outputs/merged_venture_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up venture dataset and add updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ven_df = ven_full_df.drop(['i3 URL','Seeking Funding Date','Ticker Symbol','Record Created'],axis=1)\n",
    "ven_df['company_lower'] = ven_df['Company'].apply(lambda x: x.lower())\n",
    "ven_df = ven_df[~ven_df['company_lower'].isin(excluded)]\n",
    "\n",
    "# Update year founded and filter (manual updates)\n",
    "ven_df = ven_df.merge(ven_founding_updates,left_on='company_lower',right_on = 'company', how='left')\n",
    "ven_df['Year Founded'] = ven_df.apply(lambda row: row['year.founded.update'] if pd.notnull(row['year.founded.update']) \\\n",
    "                                      else row['Year Founded'], axis = 1) \n",
    "\n",
    "ven_df = ven_df[ven_df['Year Founded'] >= 2005]\n",
    "ven_df = ven_df[ven_df['Year Founded'] < 2021] # 2100 is default when no founding year could be found\n",
    "\n",
    "# Update location and filter (manual updates from author team)\n",
    "ven_df = ven_df.merge(ven_location_updates,left_on='company_lower',right_on='Company_to_merge',how='left')\n",
    "ven_df[\"Location_source\"] = \"\"\n",
    "\n",
    "def update_location(row):\n",
    "    if pd.notnull(row['Address_update']):\n",
    "        address = row['Address_update']\n",
    "    else: address = row['Address']\n",
    "    if pd.notnull(row['City_update']):\n",
    "        city = row['City_update']\n",
    "    else: city = row['City']\n",
    "    if pd.notnull(row['State_update']):\n",
    "        state = row['State_update']\n",
    "    else: state = row['State']\n",
    "    if pd.notnull(row['Zip_update']):\n",
    "        zipc = row['Zip_update']\n",
    "    else: zipc = row['Zip']\n",
    "    if pd.notnull(row['Country_update']):\n",
    "        country = row['Country_update']\n",
    "    else: country = row['Country']\n",
    "    if pd.notnull(row['Region_update']):\n",
    "        region = row['Region_update']\n",
    "    else: region = row['Region']\n",
    "    if pd.notnull(row['Location_source']):\n",
    "        source = row['Location_source']\n",
    "    else: source = ''\n",
    "    return address, city, state, zipc, country, region, source \n",
    "\n",
    "ven_df.Address,ven_df.City,ven_df.State,ven_df.Zip,ven_df.Country,ven_df.Region,ven_df.Location_source = zip(\n",
    "    *ven_df.apply(lambda row: update_location(row), axis=1))\n",
    "\n",
    "ven_df = ven_df[ven_df['Country'] == 'United States']\n",
    "ven_df = ven_df[~ven_df['State'].isin(('ON', 'QC', 'BC', 'MB'))] # remove Canadian entries mis-labeled as USA\n",
    "\n",
    "# Update with previously cleaned data from Susanne Kurowski - failure date and zip code\n",
    "sus_df = sus_df[['startup','yearfounded','s_time_death','s_death','zip']] \n",
    "sus_df['death date'] = sus_df['yearfounded'] + sus_df['s_time_death']\n",
    "sus_df = sus_df[['startup','death date','zip']]\n",
    "sus_df['startup'] = sus_df['startup'].apply(lambda x: x.lower())\n",
    "ven_df = ven_df.merge(sus_df,left_on='company_lower',right_on = 'startup', how='left')\n",
    "\n",
    "def update_zip(row):\n",
    "    if ~np.isnan(row['zip']):\n",
    "        return row['zip'],'Susanne data'\n",
    "    return row['Zip'],row['Location_source']\n",
    "ven_df['Zip'], ven_df['Location_source'] = zip(*ven_df.apply(lambda row: update_zip(row), axis=1))\n",
    "\n",
    "## Update with manual data\n",
    "# Status\n",
    "ven_df = ven_df.merge(ven_fail_updates,on='Company',how='left')\n",
    "ven_df['Status'] = ven_df.apply(lambda row: row['Status_update'] if pd.notna(row['Status_update']) \\\n",
    "                                      else row['Status'], axis = 1)\n",
    "\n",
    "ven_df['Status'] = ven_df.Status.astype('str')\n",
    "\n",
    "def check_fail(row):\n",
    "    if row['Status'] == 'Out of Business':\n",
    "        return 1\n",
    "    if row['Status'] == 'Bankrupt':\n",
    "        return 1\n",
    "    if ~np.isnan(row['death date']):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "# Failure dates and calculate age\n",
    "#ven_df['Failure'] = ven_df.apply(lambda row: check_fail(row), axis=1)\n",
    "ven_df['Failure Date'] = ven_df['death date']\n",
    "ven_df['Failure Date'] = ven_df.apply(lambda row: row['Failure Date_update'] if pd.isnull(row['Failure Date']) \\\n",
    "                                      else row['Failure Date'], axis = 1)\n",
    "\n",
    "ven_df = ven_df.drop(columns=['death date','zip','startup','Status_update','Failure Date_update',\n",
    "                             'year.founded','year.founded.update','company','Company_to_merge',\n",
    "                             'Address_update','City_update','State_update','Zip_update','Country_update',\n",
    "                             'Region_update','Location_source','Failure Date Source'])\n",
    "ven_df = ven_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "ven_df['ID'] = ven_df.index + 1\n",
    "id_dict = dict(zip(ven_df.company_lower, ven_df.ID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up IPO, Merger/Acquisition, Corporate relationships, and Patents datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ipos_full_df = ipos_full_df[ipos_full_df['Year Founded'] >= 2005]\n",
    "ipos_df = ipos_full_df.iloc[:,[0,15,16,17,18,19,20,21,22]]\n",
    "ipos_df = ipos_df[ipos_df['IPO Status'] != 'Speculated']\n",
    "ipos_df = ipos_df[ipos_df['IPO Status'] != 'Withdrawn']\n",
    "ipos_df['IPO'] = 1\n",
    "ipos_df = ipos_df.rename(columns = {'Date':'IPO Date','Amount Raised':'IPO Amount Raised'})\n",
    "ipos_df = ipos_df[~ipos_df['IPO Date'].isna()] # Drop companies missing date after manual check\n",
    "ipos_df['IPO Date'] = ipos_df['IPO Date'].apply(lambda x: str(x)[-4:]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_df = ma_full_df.drop_duplicates(subset=['Target','Acquirer','Announced'])\n",
    "ma_df = ma_df.rename(columns={'Amount':'MA Amount','Notes':'MA Notes'})\n",
    "\n",
    "# Fill in missing announced values with 0 \n",
    "# verified manually that none of these startups are in our set of interest\n",
    "values = {\"Announced\": '0'}\n",
    "ma_df = ma_df.fillna(value=values)\n",
    "ma_df['MA'] = 1\n",
    "\n",
    "#ma_dups = ma_df[ma_df.duplicated(subset='Target',keep=False)].sort_values(by=['Target']).reset_index(drop=True)\n",
    "#ma_dups.to_csv('outputs/duplicate_ma_entries.csv',index=False, encoding = \"utf-8\")\n",
    "\n",
    "# Keep only the first MA entry for each startup\n",
    "ma_df['Announced'] = ma_df['Announced'].apply(lambda x: str(x)[:4]).astype(int)\n",
    "ma_df = ma_df.sort_values(by=['Target','Announced']).drop_duplicates(subset='Target',keep='first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_df = rel_full_df.rename(columns={'Date':'Relationship Date','Source':'Relationship Source',\n",
    "                                     'Notes':'Relationship Notes'})\n",
    "rel_df = rel_df.drop(columns=['Country 1','Country 2','Sector 1','Sector 2'])\n",
    "\n",
    "rel_a = rel_df[rel_df['Company 1'].isin(ven_df['Company'].tolist())]\n",
    "rel_b = rel_df[rel_df['Company 2'].isin(ven_df['Company'].tolist())]\n",
    "\n",
    "rel_a = rel_a.rename(columns = {'Company 1':'Company','Company 2': 'Relationship Partner'})\n",
    "rel_b = rel_b.rename(columns = {'Company 1':'Relationship Partner','Company 2': 'Company'})\n",
    "\n",
    "rel_to_merge = pd.concat([rel_a,rel_b]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "def update_rel(row):\n",
    "    if ~np.isnan(row['Relationship Date Update']):\n",
    "        return row['Relationship Date Update']\n",
    "    return row['Relationship Date']\n",
    "\n",
    "# Make a df that is just for relationship as an outcome - startup has a customer\n",
    "rel_out = rel_to_merge.copy()\n",
    "rel_out = rel_out[rel_out['Relationship Type']=='Customer']\n",
    "rel_out.loc[:,'Customer Relationship'] = 1\n",
    "rel_out = rel_out.drop_duplicates(subset='Company')\n",
    "rel_out['company_lower'] = rel_out['Company'].apply(lambda x: x.lower())\n",
    "rel_out = rel_out.merge(rel_updates,left_on = ['company_lower','Relationship Partner'], \n",
    "                        right_on = ['Company','Relationship Partner'], how='left')\n",
    "rel_out['Relationship Date'] = rel_out.apply(lambda row: update_rel(row), axis=1)\n",
    "values = {\"Relationship Date\": '2021/12/30'} \n",
    "rel_out = rel_out.fillna(value=values)\n",
    "rel_out['Relationship Date'] = rel_out['Relationship Date'].apply(lambda x: str(x)[:4]).astype(int)\n",
    "rel_out = rel_out.drop(columns=['Company_y','company_lower','Relationship Date Update'])\n",
    "rel_out = rel_out.rename(columns={'Company_x':'Company'})\n",
    "\n",
    "# Aggregate all relationship data to have a complete record\n",
    "rel_to_merge = (rel_to_merge.groupby(by='Company')\n",
    "                .agg({'Relationship Partner': lambda x: x.tolist(),\n",
    "                      'Relationship Type': lambda x: x.tolist(),\n",
    "                      'Relationship Date': lambda x: x.tolist()}).reset_index())\n",
    "rel_to_merge.loc[:,'Relationship'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total number of patents for each startup\n",
    "pat_to_merge = (pat_df.groupby(by='Company').sum().\n",
    "                drop(columns={'year'}).rename(columns={'pat_count':'patent_count'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine investment data from i3 with additional government grant data from ARPA-E and SBIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmkenne5\\AppData\\Local\\Temp\\ipykernel_13612\\1674181182.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cvc_df = (cvc_df.append(arpae_new).append(sbir_df)\n",
      "C:\\Users\\kmkenne5\\AppData\\Local\\Temp\\ipykernel_13612\\1674181182.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cvc_df = (cvc_df.append(arpae_new).append(sbir_df)\n",
      "C:\\Users\\kmkenne5\\AppData\\Local\\Temp\\ipykernel_13612\\1674181182.py:19: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  cvc_df.loc[(arpae_upd.loc[i]['recipient.renamed'],\n"
     ]
    }
   ],
   "source": [
    "# Append new grants from ARPA-E and SBIR\n",
    "arpae_new = arpae_new.rename(columns={'recipient_rename':'company'})\n",
    "arpae_new['investor.firm'] = 'arpa-e'\n",
    "cvc_df = (cvc_df.append(arpae_new).append(sbir_df)\n",
    "          .drop(columns=['recipient_name','investment.amount.total']).drop_duplicates())\n",
    "cvc_df['investment.amount'] = cvc_df['investment.amount'].astype(float)\n",
    "\n",
    "arpae_upd = arpae_upd[['recipient.renamed','investor.firm.renamed',\n",
    "                       'investment.year','investment.amount','i3.year']]\n",
    "arpae_upd['investment.year'] = arpae_upd['investment.year'].astype(float)\n",
    "arpae_upd['i3.year'] = arpae_upd['i3.year'].astype(float)\n",
    "arpae_upd['investment.amount'] = arpae_upd['investment.amount'].astype(float)\n",
    "\n",
    "# Set index for cvc_df to (company,investor.firm.renamed,investment.year)\n",
    "cvc_df = cvc_df.set_index(['company','investor.firm.renamed','investment.year']).sort_index(level=0)\n",
    "\n",
    "# Cycle through and update amounts\n",
    "for i in np.arange(0,len(arpae_upd)):\n",
    "    cvc_df.loc[(arpae_upd.loc[i]['recipient.renamed'],\n",
    "                arpae_upd.loc[i]['investor.firm.renamed'],\n",
    "                arpae_upd.loc[i]['i3.year']),'investment.amount']= arpae_upd.loc[i]['investment.amount']\n",
    "\n",
    "# Set index for cvc_df to (company,investment.amount)\n",
    "cvc_df = cvc_df.reset_index()\n",
    "cvc_df = cvc_df.set_index(['company','investor.firm.renamed','investment.amount']).sort_index(level=0)\n",
    "\n",
    "# Cycle through and update years\n",
    "for i in np.arange(0,len(arpae_upd)):\n",
    "    cvc_df.loc[(arpae_upd.loc[i]['recipient.renamed'],\n",
    "               arpae_upd.loc[i]['investor.firm.renamed'],\n",
    "               arpae_upd.loc[i]['investment.amount']),'investment.year'] = arpae_upd.loc[i]['investment.year']\n",
    "\n",
    "cvc_df = cvc_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get investment data into format for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmkenne5\\AppData\\Local\\Temp\\ipykernel_13612\\1717285432.py:51: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  inv_counts = inv_df.groupby('company').sum().reset_index()\n"
     ]
    }
   ],
   "source": [
    "# Get year founded updates from venture dataframe\n",
    "ven_merge = ven_df[['company_lower','Year Founded']].drop_duplicates()\n",
    "\n",
    "# Version for logit analysis\n",
    "inv_df = cvc_df.copy()\n",
    "\n",
    "inv_df = inv_df.merge(ven_merge, how = 'left',left_on = 'company', right_on = 'company_lower')\n",
    "inv_df = inv_df[inv_df['investment.year'] >= inv_df['Year Founded']]\n",
    "\n",
    "inv_dict = {'financial sector':'other investors', 'private equity':'other investors', \n",
    "            'venture capital':'other investors','':'',\n",
    "            'corporation or corporate venture':'corporation or corporate venture', \n",
    "            'public or quasi public':'public or quasi public',\n",
    "            'angel investor':'other investors', 'family':'other investors', \n",
    "            'other':'other investors', 'accelerator or incubator':'other investors',\n",
    "            'university':'other investors','crowdfunding':'other investors',\n",
    "            'not an investor':'other investors','other investors':'other investors',np.nan:''}\n",
    "\n",
    "inv_df['investor.type.edited'] = inv_df['investor.type.edited'].apply(lambda x: inv_dict[x])\n",
    "\n",
    "def separate_grants(row):\n",
    "    if row['investor.type.edited'] == 'public or quasi public':\n",
    "        if row['investment.type'] == 'grant':\n",
    "            return 'public or quasi public'\n",
    "        else: \n",
    "            return 'other investors'\n",
    "    else:\n",
    "        return row['investor.type.edited']\n",
    "\n",
    "inv_df['investor.type.edited'] = inv_df.apply(lambda row: separate_grants(row), axis = 1)\n",
    "\n",
    "# For each investment, make binary variable for CVC, Other, Public Grants, and ARPA-E\n",
    "def check_arpae(x):\n",
    "    if x == 'arpa-e (old)':\n",
    "        return 1\n",
    "    if x == 'arpa-e':\n",
    "        return 1\n",
    "    if x =='advanced research projects agency - energy (arpa-e)':\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "\n",
    "inv_df['ARPAE'] = inv_df['investor.firm'].apply(lambda x: check_arpae(x))\n",
    "\n",
    "inv_df = pd.get_dummies(inv_df, columns = ['investor.type.edited'])\n",
    "inv_df = inv_df.rename(columns = {'investor.type.edited_corporation or corporate venture':'CVC',\n",
    "                                  'investor.type.edited_public or quasi public':'Public_grant',\n",
    "                                  'investor.type.edited_other investors':'Other_inv'})\n",
    "\n",
    "# Groupby and get counts of each investor type\n",
    "inv_counts = inv_df.groupby('company').sum().reset_index()\n",
    "inv_counts = inv_counts[['company','CVC','Other_inv','Public_grant','ARPAE']]\n",
    "inv_counts = inv_counts.rename(columns = {'CVC':'CVC_count',\n",
    "                                         'Public_grant':'Public_grant_count',\n",
    "                                         'Other_inv':'Other_inv_count',\n",
    "                                         'ARPAE':'ARPAE_count'})\n",
    "# From counts, get binary variables\n",
    "inv_counts['CVC_bin'] = inv_counts['CVC_count'].apply(lambda x: 1 if x > 0 else 0)\n",
    "inv_counts['Public_grant_bin'] = inv_counts['Public_grant_count'].apply(lambda x: 1 if x > 0 else 0)\n",
    "inv_counts['Other_inv_bin'] = inv_counts['Other_inv_count'].apply(lambda x: 1 if x > 0 else 0)\n",
    "inv_counts['ARPAE_bin'] = inv_counts['ARPAE_count'].apply(lambda x: 1 if x > 0 else 0)\n",
    "inv_df = inv_counts.copy()\n",
    "\n",
    "\n",
    "# Combine all counts and binary varialbes to get a single line for each startup\n",
    "#inv_df = inv_counts.merge(cvc_cat_counts,on='company',how='left')\n",
    "\n",
    "# Version for recurrent event analysis\n",
    "invs_df = cvc_df.copy()\n",
    "\n",
    "invs_df = invs_df.merge(ven_merge, how = 'left',left_on = 'company', right_on = 'company_lower')\n",
    "invs_df = invs_df[invs_df['investment.year'] >= invs_df['Year Founded']]\n",
    "\n",
    "inv_dict = {'financial sector':'other investors', 'private equity':'other investors', \n",
    "            'venture capital':'other investors','':'',\n",
    "            'corporation or corporate venture':'corporation or corporate venture', \n",
    "            'public or quasi public':'public or quasi public',\n",
    "            'angel investor':'other investors', 'family':'other investors', \n",
    "            'other':'other investors', 'accelerator or incubator':'other investors',\n",
    "            'university':'other investors','crowdfunding':'other investors',\n",
    "            'not an investor':'other investors','other investors':'other investors',np.nan:''}\n",
    "invs_df['investor.type.edited'] = invs_df['investor.type.edited'].apply(lambda x: inv_dict[x])\n",
    "\n",
    "def separate_grants(row):\n",
    "    if row['investor.type.edited'] == 'public or quasi public':\n",
    "        if row['investment.type'] == 'grant':\n",
    "            return 'public or quasi public'\n",
    "        else: \n",
    "            return 'other investors'\n",
    "    else:\n",
    "        return row['investor.type.edited']\n",
    "    \n",
    "invs_df['investor.type.edited'] = invs_df.apply(lambda row: separate_grants(row), axis = 1)\n",
    "\n",
    "invs_df = (pd.get_dummies(invs_df, columns=['investor.type.edited'])\n",
    "            .groupby(['company','investment.type','investment.year'],as_index=False)\n",
    "            .agg({'investor.type.edited_corporation or corporate venture':'sum',\n",
    "                 \"investor.type.edited_other investors\":\"sum\",\n",
    "                  \"investor.type.edited_public or quasi public\":\"sum\"}))\n",
    "            \n",
    "invs_df.rename(columns={'investor.type.edited_corporation or corporate venture':\"CVC\",\n",
    "                        \"investor.type.edited_other investors\":\"Other_investors\",\n",
    "                        \"investor.type.edited_public or quasi public\":\"Pub_grant\"}, inplace=True)\n",
    "invs_df['CVC_bin'] = invs_df['CVC'].apply(lambda x: int(x != 0))\n",
    "invs_df['Other_inv_bin'] = invs_df['Other_investors'].apply(lambda x: int(x != 0))\n",
    "invs_df['Pub_grant_bin'] = invs_df['Pub_grant'].apply(lambda x: int(x != 0))\n",
    "\n",
    "\n",
    "invs_df = invs_df.merge(ven_merge, left_on = 'company', right_on = 'company_lower', how='right')\n",
    "\n",
    "invs_df['t_inv'] = invs_df['investment.year'] - invs_df['Year Founded']\n",
    "\n",
    "invs_df = invs_df.drop(columns = ['company_lower','Year Founded'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find last recorded activity for survival variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find last investment in company\n",
    "last_inv = (cvc_df.groupby('company').agg({'investment.year':'max'})\n",
    "            .reset_index().rename(columns={'company':'Company'}))\n",
    "\n",
    "# Last year a company received a patent\n",
    "last_patent = pat_df.groupby('Company').agg({'year':'max'}).reset_index()\n",
    "\n",
    "# Last year a company formed a new corporate relationship\n",
    "rel_df = rel_full_df.rename(columns={'Date':'Relationship Date','Source':'Relationship Source',\n",
    "                                     'Notes':'Relationship Notes'})\n",
    "rel_df = rel_df.drop(columns=['Country 1','Country 2','Sector 1','Sector 2'])\n",
    "\n",
    "rel_a = rel_df[rel_df['Company 1'].isin(ven_df['Company'].tolist())]\n",
    "rel_b = rel_df[rel_df['Company 2'].isin(ven_df['Company'].tolist())]\n",
    "\n",
    "rel_a = rel_a.rename(columns = {'Company 1':'Company','Company 2': 'Relationship Partner'})\n",
    "rel_b = rel_b.rename(columns = {'Company 1':'Relationship Partner','Company 2': 'Company'})\n",
    "\n",
    "last_rel = pd.concat([rel_a,rel_b]).drop_duplicates().reset_index(drop=True)\n",
    "last_rel = last_rel.dropna(subset=['Relationship Date'])\n",
    "last_rel['Relationship Date'] = last_rel['Relationship Date'].apply(lambda x: str(x)[:4]).astype(int)\n",
    "first_rel = last_rel.groupby('Company').agg({'Relationship Date':'min'}).reset_index()\n",
    "last_rel = last_rel.groupby('Company').agg({'Relationship Date':'max'}).reset_index()\n",
    "last_rel['Company'] = last_rel['Company'].apply(lambda x: x.lower())\n",
    "\n",
    "# Last year company experienced a success event\n",
    "last_ipo = ipos_df.groupby('Company').agg({'IPO Date':'max'}).reset_index()\n",
    "last_ipo['Company'] = last_ipo['Company'].apply(lambda x: x.lower())\n",
    "last_ma = ma_df.groupby('Target').agg({'Announced':'max'}).reset_index()\n",
    "last_ma['Company'] = last_ma['Target'].apply(lambda x: x.lower())\n",
    "\n",
    "\n",
    "last_action = (last_ipo.merge(last_patent, on = 'Company', how = 'outer')\n",
    "               .merge(last_rel, on='Company', how='outer')\n",
    "               .merge(last_ma, on='Company', how='outer')\n",
    "               .merge(last_inv, on='Company', how='outer'))\n",
    "last_action = last_action.fillna(0)\n",
    "last_action['last_act'] = last_action.apply(lambda row: max(row['IPO Date'],row['Relationship Date'],\n",
    "                                                            row['year'],row['Announced'],\n",
    "                                                            row['investment.year']),axis=1)\n",
    "last_action = last_action.drop_duplicates()\n",
    "last_action = last_action.drop(columns = ['IPO Date','year','Relationship Date',\n",
    "                                          'Announced','Target','investment.year'])\n",
    "last_action = last_action.rename(columns = {'Company':'company_lower'})\n",
    "#last_action.to_csv('outputs/last_acts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all datasets for hazards analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "hazard_df = ven_df.merge(ipos_df,on='Company',how='left')\n",
    "hazard_df = hazard_df.merge(ma_df,left_on='Company',right_on='Target',how='left')\n",
    "hazard_df = hazard_df.merge(rel_out,on='Company', how='left')\n",
    "hazard_df['Company'] = hazard_df['Company'].apply(lambda x: x.lower())\n",
    "hazard_df = hazard_df.merge(pat_to_merge,on='Company',how='left')\n",
    "#inv_df for single line, invs_df for multi-line recurrent event analysis\n",
    "hazard_df = hazard_df.merge(invs_df,left_on = 'Company', right_on = 'company',how='inner') \n",
    "hazard_df = hazard_df.merge(ven_updates, left_on='Company',right_on='orgname',how='left')\n",
    "hazard_df = hazard_df.merge(last_action,on='company_lower',how='left')\n",
    "hazard_df = hazard_df.merge(inactive_updates,on='Company',how='left')\n",
    "\n",
    "# Fill in last action with founding year if blank\n",
    "hazard_df['last_act'] = hazard_df.apply(lambda row: row['Year Founded'] if pd.isna(row['last_act']) \n",
    "                                        else row['last_act'], axis=1)\n",
    "\n",
    "def update_status(row):\n",
    "    if pd.notnull(row['Status_update']):\n",
    "        status = row['Status_update']\n",
    "    else: status = row['Status']\n",
    "    if pd.notnull(row['Failure_date']):\n",
    "        fail_date = int(row['Failure_date'])\n",
    "    else: fail_date = row['Failure Date']\n",
    "    if pd.notnull(row['IPO_date']):\n",
    "        ipo_date = int(row['IPO_date'])\n",
    "    else: ipo_date = row['IPO Date']\n",
    "    if pd.notnull(row['MA_date']):\n",
    "        ma_date = int(row['MA_date'])\n",
    "    else: ma_date = row['Announced']\n",
    "    if pd.notnull(row['Acquirer_update']):\n",
    "        acquirer = row['Acquirer_update']\n",
    "    else: acquirer = row['Acquirer']\n",
    "    return status, fail_date, ipo_date, ma_date, acquirer\n",
    "\n",
    "hazard_df.Status,hazard_df['Failure Date'],hazard_df['IPO Date'],hazard_df.Announced,hazard_df.Acquirer = zip(\n",
    "    *hazard_df.apply(lambda row: update_status(row), axis=1))\n",
    "\n",
    "# Update ma column with Susanne's data\n",
    "def check_ma(row):\n",
    "    if row['MA'] == 1:\n",
    "        return 1\n",
    "    if row['MA_update'] == 1:\n",
    "        return 1\n",
    "    if pd.notnull(row['Announced']):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "hazard_df['MA'] = hazard_df.apply(lambda row: check_ma(row), axis=1)\n",
    "\n",
    "def check_ipo(row):\n",
    "    if row['IPO'] == 1:\n",
    "        return 1\n",
    "    if pd.notnull(row['IPO Date']):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "hazard_df['IPO'] = hazard_df.apply(lambda row: check_ipo(row), axis=1)\n",
    "\n",
    "def check_fail2(row):\n",
    "    if row['Status'] == 'Out of Business':\n",
    "        return 1\n",
    "    if row['Status'] == 'Bankrupt':\n",
    "        return 1\n",
    "    if pd.notnull(row['Failure Date']):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "hazard_df['Failure'] = hazard_df.apply(lambda row: check_fail2(row), axis = 1)\n",
    "\n",
    "values = {'patent_count':0, 'IPO': 0, 'MA': 0, 'Relationship': 0,'sbti.status':0,'Failure':0,\n",
    "          'Customer Relationship':0}\n",
    "hazard_df = hazard_df.fillna(value=values)\n",
    "\n",
    "\n",
    "def calc_success(row):\n",
    "    if (row.IPO==1) and (row.MA==1):\n",
    "        success = 1\n",
    "        success_year = min(row['Announced'], row['IPO Date'])\n",
    "        return success, success_year\n",
    "    if (row.IPO==1):\n",
    "        success = 1\n",
    "        success_year = row['IPO Date']\n",
    "        return success, success_year\n",
    "    if (row.MA==1):\n",
    "        success = 1\n",
    "        success_year = row['Announced']\n",
    "        return success, success_year\n",
    "    success = 0\n",
    "    success_year = ''  # Change to '' for multi-line recurrent event analysis\n",
    "    return success, success_year\n",
    "    \n",
    "def calc_times(row,col):\n",
    "    try:\n",
    "        return row[col] - row['Year Founded']\n",
    "    except:\n",
    "        return ''\n",
    "    \n",
    "def calc_outcome(row):\n",
    "    if row['Success'] == 1 and row['Failure'] == 1:\n",
    "        return min(row['time_to_success'],row['time_to_failure'])\n",
    "    if row['Success'] == 1:\n",
    "        return row['time_to_success']\n",
    "    if row['Failure'] == 1:\n",
    "        return row['time_to_failure']\n",
    "    else:\n",
    "        return ''\n",
    "        \n",
    "# Make time_to_success, time_to_failure, time_to_customer variables\n",
    "hazard_df['Success'], hazard_df['Success_year'] = zip(*hazard_df.apply(lambda row: calc_success(row), axis= 1))\n",
    "hazard_df['time_to_success'] = hazard_df.apply(lambda row: calc_times(row,'Success_year'), axis = 1)\n",
    "hazard_df['time_to_failure'] = hazard_df.apply(lambda row: calc_times(row,'Failure Date'), axis = 1)\n",
    "hazard_df['time_to_rel'] = hazard_df.apply(lambda row: calc_times(row,'Relationship Date'), axis = 1)\n",
    "hazard_df['time_to_ipo'] = hazard_df.apply(lambda row: calc_times(row,'IPO Date'), axis = 1)\n",
    "hazard_df['time_to_ma'] = hazard_df.apply(lambda row: calc_times(row,'Announced'), axis = 1)\n",
    "hazard_df['time_to_outcome'] = hazard_df.apply(lambda row: calc_outcome(row), axis = 1)\n",
    "\n",
    "# Make location variable\n",
    "# Make location binary variable = 1 if in Massachusettes, California, Colorado, DC as the hot spots from map\n",
    "state_list = ['California','Colorado','Massachusetts','Washington DC']\n",
    "hazard_df['Location'] = hazard_df['State'].apply(lambda x: 1 if x in state_list else 0)\n",
    "\n",
    "def calc_age(row):\n",
    "    if row['Failure'] == 1:\n",
    "        return (int(row['Failure Date'])- int(row['Year Founded']) + 1)\n",
    "    return (2021 - int(row['Year Founded']) + 1)\n",
    "\n",
    "# Drop startups that only received investment in 2021\n",
    "too_late = hazard_df[['ID','Company','investment.year']].groupby('Company').min()\n",
    "too_late = too_late[too_late['investment.year'] > 2020]\n",
    "too_late_list = too_late['ID'].tolist()\n",
    "hazard_df = hazard_df[~hazard_df['ID'].isin(too_late_list)]\n",
    "\n",
    "# Drop startups that only received investment before their founding date\n",
    "too_early = hazard_df[['ID','Company','Year Founded','investment.year']].groupby('Company').max()\n",
    "too_early = too_early[too_early['investment.year'] < too_early['Year Founded']]\n",
    "too_early_list = too_early['ID'].tolist()\n",
    "hazard_df = hazard_df[~hazard_df['ID'].isin(too_early_list)]\n",
    "\n",
    "hazard_df['age'] = hazard_df.apply(lambda row: 2021 - row['Year Founded'], axis =1)\n",
    "\n",
    "# Drop unneeded columns\n",
    "hazard_df = hazard_df.drop(columns = ['Revenue Range','Revenue Range Source','Nbr Employees Range',\n",
    "                                    'IPO Status','IPO Type','IPO Amount Raised','Shares Offered at IPO',\n",
    "                                    'MA Amount','MA Notes','Relationship Source','Relationship Notes',\n",
    "                                     'Status_update','Failure_date','IPO_date','MA_date','Acquirer_update',\n",
    "                                     'MA_update','company_lower','orgname','Target','Failure_update'])\n",
    "\n",
    "hazard_df.to_csv('outputs/hazard_data_recurrent.csv', index=False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all datasets together for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = ven_df.merge(ipos_df,on='Company',how='left')\n",
    "total_df = total_df.merge(ma_df,left_on='Company',right_on='Target',how='left')\n",
    "total_df = total_df.merge(rel_out,on='Company', how='left')\n",
    "total_df['Company'] = total_df['Company'].apply(lambda x: x.lower())\n",
    "total_df = total_df.merge(pat_to_merge,on='Company',how='left')\n",
    "total_df = total_df.merge(ven_updates, left_on='company_lower',right_on='orgname',how='left')\n",
    "\n",
    "total_df = total_df.drop(columns = ['MA_update','company_lower','orgname','Target','Failure_update'])\n",
    "total_df = total_df.rename(columns={'country':'investor.country'})\n",
    "\n",
    "values = {\"IPO\": 0, \"MA\": 0, \"Relationship\": 0,'sbti.status':0,'Failure':0}\n",
    "total_df = total_df.fillna(value=values)\n",
    "\n",
    "def separate_grants(row):\n",
    "    if row['investor.type.edited'] == 'public or quasi public':\n",
    "        if row['investment.type'] == 'grant':\n",
    "            return 'public or quasi public'\n",
    "        else: \n",
    "            return 'other investors'\n",
    "    else:\n",
    "        return row['investor.type.edited']\n",
    "\n",
    "inv_dict = {'financial sector':'other investors', 'private equity':'other investors', \n",
    "            'venture capital':'other investors','':'',\n",
    "            'corporation or corporate venture':'corporation or corporate venture', \n",
    "            'public or quasi public':'public or quasi public',\n",
    "            'angel investor':'other investors', 'family':'other investors', \n",
    "            'other':'other investors', 'accelerator or incubator':'other investors',\n",
    "            'university':'other investors','crowdfunding':'other investors',\n",
    "            'not an investor':'other investors','other investors':'other investors',np.nan:''}\n",
    "cvc_df['investor.type.edited'] = cvc_df['investor.type.edited'].apply(lambda x: inv_dict[x])\n",
    "\n",
    "cvc_df['investor.type.edited'] = cvc_df.apply(lambda row: separate_grants(row), axis = 1)\n",
    "\n",
    "total_df = total_df.merge(cvc_df,left_on = 'Company', right_on = 'company',how='left')\n",
    "\n",
    "# Drop investments in 2021 to align with CPH methodology\n",
    "total_df = total_df[total_df['investment.year'] < 2021]\n",
    "\n",
    "# Drop investments before startup founding date\n",
    "total_df = total_df[total_df['investment.year'] >= total_df['Year Founded']]\n",
    "\n",
    "total_df.to_csv(\"outputs/merged_venture_data.csv\", index=False, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary numbers from final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Companies: 3118\n",
      "Number of Patents: 18214\n"
     ]
    }
   ],
   "source": [
    "#print('Number of Investors: ' + str(len(cvc_df['investor.firm.renamed'].unique())))\n",
    "print('Number of Companies: ' + str(len(hazard_df['Company'].unique())))\n",
    "#print('Number of Deals: ' + str(len(inv_df[~inv_df['investor.firm.renamed'].isna()])))\n",
    "print('Number of Patents: ' + str(pat_df['pat_count'].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of companies in final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tots = total_df.iloc[:,:14].drop_duplicates().reset_index(drop=True)\n",
    "tots.to_csv(\"outputs/companies_from_merged_data.csv\", index=False, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
